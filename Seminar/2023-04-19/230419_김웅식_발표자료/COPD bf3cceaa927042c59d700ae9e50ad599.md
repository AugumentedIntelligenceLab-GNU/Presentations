# COPD

- COPD
    
    ```python
    from sklearn.model_selection import train_test_split
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.svm import SVC
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.naive_bayes import GaussianNB
    from sklearn.metrics import confusion_matrix
    import xgboost as xgb
    from imblearn.over_sampling import SMOTE
    
    class COPD:
        def __init__(self, dataset, **options):
            self.dataset = dataset.copy()
            self.X_train = None
            self.y_train = None
            self.X_test = None
            self.y_test = None
            self.normalize_y()
            if options["process_age"]:
                self.process_age()
            self.train_test_split()
            if options["oversampling"]:
                self.oversampling()
    
        def normalize_y(self):
            self.dataset.loc[self.dataset["AS1_COPD_D"] == 1, "AS1_COPD_D"] = 0
            self.dataset.loc[self.dataset["AS1_COPD_D"] == 2, "AS1_COPD_D"] = 1
    
        def process_age(self):
            # print(f"나이변수 처리 전: \n {self.dataset.shape}")
            self.dataset = self.dataset[self.dataset["AS1_AGE"] >= 60]
            self.dataset = self.dataset.drop(["AS1_AGE"], axis=1)
            # print(f"나이변수 처리 후: \n {self.dataset.shape}")
    
        def train_test_split(self):
            dataset1 = self.dataset[self.dataset["AS1_COPD_D"] == 0]
            dataset2 = self.dataset[self.dataset["AS1_COPD_D"] == 1]
    
            dataset1_splited = train_test_split(
                dataset1.drop(["AS1_COPD_D"], axis=1),
                dataset1["AS1_COPD_D"],
                test_size=125,
            )
            dataset2_splited = train_test_split(
                dataset2.drop(["AS1_COPD_D"], axis=1),
                dataset2["AS1_COPD_D"],
                test_size=125
            )
    
            self.X_train = pd.concat([dataset1_splited[0], dataset2_splited[0]])
            self.X_test = pd.concat([dataset1_splited[1], dataset2_splited[1]])
            self.y_train = pd.concat([dataset1_splited[2], dataset2_splited[2]])
            self.y_test = pd.concat([dataset1_splited[3], dataset2_splited[3]])
    
        def oversampling(self):
            smote = SMOTE()
            # print(f"Before OverSampling: \n0: {(self.y_train == 0).sum()} \n1: {(self.y_train == 1).sum()}")
            self.X_train, self.y_train = smote.fit_resample(self.X_train, self.y_train)
            # print(f"After OverSampling: \n0: {(self.y_train == 0).sum()} \n1: {(self.y_train == 1).sum()}")
            #
            # print(f"학습에 사용하는 데이터 {self.X_train.shape} {self.y_train.shape}")
    
        def calc(self, clf, mode):
            if mode == "train":
                [[tn, fp], [fn, tp]] = confusion_matrix(self.y_train, clf.predict(self.X_train))
            else:
                clf.predict(self.X_test)
                [[tn, fp], [fn, tp]] = confusion_matrix(self.y_test, clf.predict(self.X_test))
    
            precision = tp / (tp + fp)
            recall = tp / (tp + fn)
            f1 = 2 * precision * recall / (precision + recall)
            accuracy = (tp + tn) / (tp + tn + fn + fp)
    
            return pd.DataFrame({
                "tn": tn,
                "fp": fp,
                "fn": fn,
                "tp": tp,
                "accuracy": accuracy,
                "precision": precision,
                "recall": recall,
                "f1_score": f1
            }, index=["train" if mode == "train" else "test"])
    
        def svm(self):
            clf = SVC(kernel="linear")
            clf.fit(self.X_train, self.y_train)
            return [self.calc(clf, "train"), self.calc(clf, "test")]
    
        def DecisionTree(self):
            clf = DecisionTreeClassifier(max_depth=5)
            clf.fit(self.X_train, self.y_train)
            return [self.calc(clf, "train"), self.calc(clf, "test")]
    
        def NaiveBayes(self):
            clf = GaussianNB()
            clf.fit(self.X_train, self.y_train)
            return [self.calc(clf, "train"), self.calc(clf, "test")]
    
        def KNN(self):
            clf = KNeighborsClassifier()
            clf.fit(self.X_train, self.y_train)
            return [self.calc(clf, "train"), self.calc(clf, "test")]
    
        def XGBoost(self):
            clf = xgb.XGBClassifier()
            clf.fit(self.X_train, self.y_train)
            return [self.calc(clf, "train"), self.calc(clf, "test")]
    ```
    
- handler
    
    ```python
    class Handler():
        def __init__(self, n: int, dataset: pd.DataFrame, oversampling, process_age):
            self.n = n
            self.copd_n = [COPD(dataset, oversampling=oversampling, process_age=process_age) for _ in range(n)]
            self.train = pd.DataFrame()
            self.test = pd.DataFrame()
            self.mean = pd.Series()
    
        def init(self):
            self.train = pd.DataFrame()
            self.test = pd.DataFrame()
            self.mean = pd.Series()
    
        def svm(self):
            self.init()
            for copd in self.copd_n:
                self.train = pd.concat([self.train, copd.svm()[0]])
                self.test = pd.concat([self.test, copd.svm()[1]])
            self.mean = self.test.mean(axis=0)
    
        def tree(self):
            self.init()
            for copd in self.copd_n:
                self.train = pd.concat([self.train, copd.DecisionTree()[0]])
                self.test = pd.concat([self.test, copd.DecisionTree()[1]])
            self.mean = self.test.mean(axis=0)
    
        def nb(self):
            self.init()
            for copd in self.copd_n:
                self.train = pd.concat([self.train, copd.NaiveBayes()[0]])
                self.test = pd.concat([self.test, copd.NaiveBayes()[1]])
            self.mean = self.test.mean(axis=0)
    
        def xgb(self):
            self.init()
            for copd in self.copd_n:
                self.train = pd.concat([self.train, copd.XGBoost()[0]])
                self.test = pd.concat([self.test, copd.XGBoost()[1]])
            self.mean = self.test.mean(axis=0)
    
        def knn(self):
            self.init()
            for copd in self.copd_n:
                self.train = pd.concat([self.train, copd.KNN()[0]])
                self.test = pd.concat([self.test, copd.KNN()[1]])
            self.mean = self.test.mean(axis=0)
    ```
    

[Decision Tree (SMOTE)](COPD%20bf3cceaa927042c59d700ae9e50ad599/Decision%20Tree%20(SMOTE)%20b9ea0181dc1e4106871f5191757a85ca.md)

[XGBoost (SMOTE)](COPD%20bf3cceaa927042c59d700ae9e50ad599/XGBoost%20(SMOTE)%20b51b1752f339426697dc600939f30368.md)

[KNN (SMOTE)](COPD%20bf3cceaa927042c59d700ae9e50ad599/KNN%20(SMOTE)%204c275e877331441f898126f4f76ee49e.md)

[SVM](COPD%20bf3cceaa927042c59d700ae9e50ad599/SVM%2022d9ad9fe03541b2b1610a65752ac4fe.md)

[Naive Bayes (SMOTE)](COPD%20bf3cceaa927042c59d700ae9e50ad599/Naive%20Bayes%20(SMOTE)%2073dde13f9d4f42c0acee7f4ce4be43ca.md)

## OverSampling

COPD를 안걸린 사람쪽으로 편향된 데이터를 해결하기 위해 OverSampling기법을 사용하여 COPD 걸린사람의 데이터를 늘림.

학습 데이터를 늘리기 위해서 SMOTE 기법을 사용함. OverSampling 기법 중 SMOTE 외에도 Borderline-SMOTE와 ADASYN이 있지만 사용했을때 성능에 큰 변화는 없다는 글을 보고 바꿔서 정확도를 측정해보지는 않음.

[SMOTE 설명](COPD%20bf3cceaa927042c59d700ae9e50ad599/SMOTE%20%E1%84%89%E1%85%A5%E1%86%AF%E1%84%86%E1%85%A7%E1%86%BC%20f9d4bea5866c49e3a58b383c50242a59.md)

## 전체적인 모델 측정 후, 견해

SMOTE를 사용했을때가 사용하지 않았을 때와 비교했을때, F1-Score에서 큰 차이를 보여줌.  특히  XGBoost는 train 데이터에 과적합되는 반면에, test 데이터에서 높은 성능이 나오지 않아 학습률, 학습횟수, 트리 층수, L1, L2등을 고려해 과적합을 낮추는 방법을 생각해보아야할 것 같음. 현재 모델예측에 사용하는 데이터는 인수인계 과정에서 데이터 전처리가 된 채로 받은 데이터를 사용하고 있으나 학습 데이터들이 결측값 처리만 되어있고 정규화가 되어있지 않아서 예측을 향상시키기 위해 정규화, 표준화의 작업 등을 거칠 필요가 있어보임. 그 외에도 다른 모델들도 사용해보는 것도 좋아보임

## 요약

1. 데이터 전처리
2. XGBoost 튜닝 (과적합이 심하기 때문에)
3. 다른 앙상블 기법 찾아보기